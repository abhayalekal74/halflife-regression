# -*- coding: utf-8 -*-
"""topic-hlr-train.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1P6QsFOhU8pusyMXomLsjCkfyQW7exD5H
"""

import pandas as pd
from datetime import datetime as dt
import math
import random
from collections import defaultdict, namedtuple
import sys
from tqdm import tqdm
import json

MIN_REC = 0.0001
MAX_REC = 0.9999
MIN_HL = 1 #1 minute
MAX_HL = 30 #30 minutes
LN2 = math.log(2.)

Instance = namedtuple('Instance', ['recall', 'hl', 'time_delta', 'feature_vector'])


def to_minutes(millis):
	return (millis / 1000.0) / 60.0


def recall_clip(recall):
	return min(max(MIN_REC, recall), MAX_REC)


def halflife_clip(halflife):
	return min(max(MIN_HL, halflife), MAX_HL)


def read_data(df):
	print ("Reading data...")
	data = list()
	for groupid, groupdf in df.groupby(['userid', 'examid']):
		prev_session_end = None
		userid = groupid[0]
		examid = groupid[1]
		for sessionid, session_group in groupdf.groupby(['hour', 'minute']):
			session_name = "{}-{}".format(sessionid[0], sessionid[1])
			total_attempts = len(session_group)
			correct_df = session_group[session_group['iscorrect'] == True]
			correct_attempts = len(correct_df)
			actual_recall = recall_clip(float(correct_df['difficulty'].sum()) / session_group['difficulty'].sum())
			session_begin_time = session_group['attempttime'].min()
			lag_time = 0 if not prev_session_end else to_minutes(session_begin_time - prev_session_end)
			actual_halflife = MIN_HL if lag_time == 0 else halflife_clip(-lag_time / math.log(actual_recall, 2))
			prev_session_end = session_group['attempttime'].max()
			data.append([userid, examid, session_name, prev_session_end, actual_recall, lag_time, actual_halflife, total_attempts, correct_attempts])
	return data


def get_instances_from_data(data):
	instances = list()
	for datum in data:
		recall = datum[4]
		hl = datum[6]
		time_delta = datum[5]
		feature_vector = list()
		feature_vector.append((sys.intern('right'), math.sqrt(1 + datum[-1])))
		feature_vector.append((sys.intern('wrong'), math.sqrt(1 + datum[-2] - datum[-1])))
		instances.append(Instance(recall, hl, time_delta, feature_vector))
	splitpoint = int(0.9 * len(instances))
	trainset = instances[:splitpoint]
	rest = instances[splitpoint:]
	rest_split = int(0.9 * len(rest))
	testset = rest[:rest_split]
	validationset = rest[rest_split:]
	return trainset, testset, validationset 


class HLRModel(object):
	def __init__(self, initial_weights=None, lrate=.001, hlwt=.01, l2wt=.1, sigma=1.):
		self.weights = defaultdict(float)
		if initial_weights is not None:
			self.weights.update(initial_weights)
		self.best_weights = None
		self.fcounts = defaultdict(int)
		self.lrate = lrate
		self.hlwt = hlwt
		self.l2wt = l2wt
		self.sigma = sigma
		self.min_val_loss = float("inf") 


	def halflife(self, inst, base):
		# h = 2 ** (theta . x)
		try:
			theta_x_dot_product = sum([self.best_weights[feature]*value for (feature, value) in inst.feature_vector])
			return halflife_clip(base ** theta_x_dot_product) 
		except:
			return MAX_HL


	def predict(self, inst, base=2.):
		halflife = self.halflife(inst, base)
		recall = 2. ** (-inst.time_delta / halflife)
		return recall_clip(recall), halflife


	def get_validation_loss(self, validationset):
		validation_loss = 0.0
		for inst in validationset:
			slr, slh, recall, hl = self.losses(inst)
			validation_loss += slr + slh
		return validation_loss


	def train_update(self, inst, validationset):
		base = 2.
		recall, hl = self.predict(inst, base)
		dl_recall_dw = 2. * (recall - inst.recall) * (LN2 ** 2) * recall * (inst.time_delta / hl)
		dl_hl_dw = 2. * (hl - inst.hl) * LN2 * hl
		for (feature, value) in inst.feature_vector:
			rate = (1. / (1 + inst.recall)) * self.lrate / math.sqrt(1 + self.fcounts[feature])
			self.weights[feature] -= rate * dl_recall_dw * value
			self.weights[feature] -= rate * self.hlwt * dl_hl_dw * value
			# L2 regularization update
			self.weights[feature] -= rate * self.l2wt * self.weights[feature] / self.sigma ** 2
			# increment feature count for learning rate
			self.fcounts[feature] += 1
		val_loss = self.get_validation_loss(validationset)			
		if val_loss < self.min_val_loss:
			self.min_val_loss = val_loss
			self.best_weights = self.weights

	
	def train(self, trainset, validationset, epochs=20):
		for i in tqdm(range(epochs), desc="Epoch "):
			for inst in tqdm(trainset, desc="Training Instance "):
				self.train_update(inst, validationset)
			with open(sys.argv[2], 'a') as f:
				f.write("\n\nEpoch {}: val_loss {}\n".format(i, self.min_val_loss))
				f.write(json.dumps(self.best_weights))
			

	
	def losses(self, inst):
		recall, hl = self.predict(inst)
		stop_loss_recall = (inst.recall - recall) ** 2
		stop_loss_hl = (inst.hl - hl) ** 2
		return stop_loss_recall, stop_loss_hl, recall, hl


	def eval(self, testset):
		print ("Predicting...")
		for inst in testset:
			sl_recall, sl_hl, recall, hl = self.losses(inst)
			print ("actual_rec {}, pred_rec {}, actual_hl {}, pred_hl {}, sl_rec {}, sl_hl {}".format(inst.recall, recall, inst.hl, hl, sl_recall, sl_hl))


if __name__=='__main__':
	df = pd.read_csv(sys.argv[1])
	df['hour'] = df.apply(lambda row: dt.fromtimestamp(row['attempttime']/1000).hour, axis=1) 
	df['minute'] = df.apply(lambda row: dt.fromtimestamp(row['attempttime']/1000).minute, axis=1)
	df = df.sort_values(by=['attempttime'], ascending=True)

	data = read_data(df)
	trainset, testset, validationset = get_instances_from_data(data)
	
	model = HLRModel()
	model.train(trainset, validationset)
	model.eval(testset)

